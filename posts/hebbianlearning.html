<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Carlo Meroni website">

    <title>Carlo Meroni website</title>

    <!-- Bootstrap Core CSS -->
    <link href="/css/bootstrap.min.css" rel="stylesheet">
    
	<!-- overflow theme -->
	<link href="/css/overflow.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
    
	<!-- jQuery -->
    <script src="/js/jquery.js"></script>
	
    <!-- Bootstrap Core JavaScript -->
    <script src="/js/bootstrap.min.js"></script>
	
	<!-- Load maincolumn -->
	<script> $(function(){ $("#maincolumn").load("/maincolumn.html");}); </script> 
	
	<!-- highlight.js -->
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/7.3/highlight.min.js"></script>
    <link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/7.3/styles/default.min.css" rel="stylesheet" />
    <script>hljs.initHighlightingOnLoad();</script>

	<!-- MathJax for inline math -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>
	<script type="text/javascript" async
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
	</script>
</head>

<body>
	
	<!-- maincolumn -->
	<div id="maincolumn"></div>

	<!-- posts -->
	<div class="pagecolumn">
	<h1>Hebbian Learning</h1>
	<p>Thoughts and experiments on artificial neural networks</p>
	<small>Published on 2 May 2016</small>
	<hr>
	<p>Hy, I'm writing this post because I want to share some experiments on artificial neural 
	networks that I'm doing in my free time. I have been using artificial neural networks
	for about a year now and I successfully used a convolutional neural network to find and 
	recognize cars license plates from images. 
	<br>Recently I started to think about the differences between biological and artificial neural networks
	because I believe that the field of artificial intelligence have a lot to learn from 
	the biological nervous system.
	Although there has been great progress in this field in recent years,
	there are some big problems yet to solve.
	</p>
	<p>
	One of the biggest problems of ANNs is <b>generalization</b>, infact ANN are pretty bad at generalizing
	what they learn, you need a lot of data to make them work well.
	Also they are very very <b>slow</b> at learning new things compared to the Human brain, for
	example Humans are able to learn to recognize an object by watching it one or few times
	while an ANN require training on hundreds or thousands of images.
	I will talk a bit more about this problem later in this post and I will explain why I believe
	we are able to learns new things so quickly and why current ANNs are not able to do that.
	Another problem in current artificial neural networks is that they can't be extended in an efficient
	way. For example let's say I want to train an ANN to recognize handwritten digits but I
	decide to train it only with images of numbers. I will end up
	with a network that is able to recognize numbers but not letters obviously.
	Let's say now that I changed my mind and I want to train the same network to recognize
	letters and numbers, so I train the network with images of letters and
	this time I end up with an ANN that is able to recognize letters but not numbers, infact the
	problem is that <b>ANNs tends to overwrite what they learned before</b>.
	The common approach is to train the network with all the data at the same time but that it's
	not the way the biological nervous systems works, we are able to learn new things without the need
	of forgetting other things.
	There are a lot of other differences but the last one I will cover is the "learning rule".
	<br>
	Let me explain: in order for learning to occur in both biological and artificial neural networks
	each neuron must follow some precise rules for deciding how to modify the connections with other neurons.
	In ANN the <b>backpropagation algorithm</b> is commonly used.
	This algorithm is very powerful, it's the holy grail of artificial intelligence,
	infact it's used to train ANNs for speech recognition, image recognition, object detection,
	email filters, self driving cars and for solving many other problems.
	While this algorithm is very useful, I believe that the problems of current ANNs that I listed
	before (bad generalization, slow learning, bad at continuous learning) are caused by 
	how this algorithm works. To demonstrate this I must show you another learning algorithm that
	is biologically plausible, known as Hebbian learning.
	</p>
	
	<h2>The Hebbian learning rule</h2>
	<p>
	The Hebbian learning rule was Introduced by Donald Hebb in his 1949 book "The Organization of Behavior".
	In his book he state as follows:
	</p>
	<blockquote>
	Let us assume that the persistence or repetition of a reverberatory activity 
	tends to induce lasting cellular changes that add to its stability. When an axon 
	of cell A is near enough to excite a cell B and repeatedly or persistently takes part in 
	firing it, some growth process or metabolic change takes place in one or both cells 
	such that A's efficiency, as one of the cells firing B, is increased.
	</blockquote>
	<p>
	What does this mean?<br>	
	Basically the hypothesis is that after a neuron fires the connections between that neuron
	and the neurons that caused it to fire become stronger.
	One of the mathematical expressions of the Hebbian learning rule is the following:
	</p>
	<div class="formula">
	$$w_{i} = w_{i} + (x_{i} - w_{i}) * learningrate$$
	</div>
	<p>
	Where $x_{i}$ are the inputs to the neuron, $w_{j}$ are the weights of the connections and
	the learningrate is a real value between 0 and 1 that determinate the learning speed.
	This means that the weights of the neuron shift in the direction of the input, basically
	the neuron learns to fire when there is a particular input.
	
	</p>
	</div>

</body>

</html>
